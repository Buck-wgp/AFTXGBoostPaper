{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "from   sklearn.model_selection import KFold\n",
    "import optuna\n",
    "import time\n",
    "import json\n",
    "from   optuna.samplers import TPESampler\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(data_name):\n",
    "    filename = 'https://raw.githubusercontent.com/avinashbarnwal/GSOC-2019/master/AFT/test/data/neuroblastoma-data-master/data/'+data_name+'/'\n",
    "    inputFileName = filename+'inputs.csv'\n",
    "    labelFileName = filename+'outputs.csv'\n",
    "    foldsFileName = filename+'cv/equal_labels/folds.csv'\n",
    "    inputs        = pd.read_csv(inputFileName,index_col='sequenceID')\n",
    "    labels        = pd.read_csv(labelFileName,index_col='sequenceID')\n",
    "    folds         = pd.read_csv(foldsFileName,index_col='sequenceID')\n",
    "    res           = {}\n",
    "    res['inputs'] = inputs\n",
    "    res['labels'] = labels\n",
    "    res['folds']  = folds\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_massage(inputs,labels):\n",
    "    inputs.replace([-float('inf'),float('inf')],np.nan,inplace=True)\n",
    "    missingCols = inputs.isnull().sum()\n",
    "    missingCols = list(missingCols[missingCols>0].index)\n",
    "    inputs.drop(missingCols,axis=1,inplace=True)\n",
    "    varCols     = inputs.apply(lambda x: np.var(x))\n",
    "    zeroVarCols = list(varCols[varCols==0].index)\n",
    "    inputs.drop(zeroVarCols,axis=1,inplace=True)\n",
    "    labels['min.log.lambda'] = labels['min.log.lambda'].apply(lambda x: np.exp(x))\n",
    "    labels['max.log.lambda'] = labels['max.log.lambda'].apply(lambda x: np.exp(x))\n",
    "    return inputs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(foldNo,folds,inputs,labels):\n",
    "    test_id       = list(folds[folds['fold']==foldNo].index)\n",
    "    train_id      = list(folds[folds['fold']!=foldNo].index)\n",
    "    X             = inputs[inputs.index.isin(train_id)]\n",
    "    X_val         = inputs[inputs.index.isin(test_id)]\n",
    "    y_label       = labels[labels.index.isin(train_id)]\n",
    "    y_label_test  = labels[labels.index.isin(test_id)]\n",
    "    y_lower       = y_label['min.log.lambda']\n",
    "    y_upper       = y_label['max.log.lambda']\n",
    "    y_lower_val   = y_label_test['min.log.lambda']\n",
    "    y_upper_val   = y_label_test['max.log.lambda']\n",
    "    res           = {}\n",
    "    res['X']         = X\n",
    "    res['X_val']     = X_val\n",
    "    res['y_lower']      = y_lower\n",
    "    res['y_lower_val']  = y_lower_val\n",
    "    res['y_upper']      = y_upper\n",
    "    res['y_upper_val']  = y_upper_val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X,X_val,y_lower,y_upper,y_lower_val,y_upper_val,params,num_round,distributionCol):\n",
    "    \n",
    "    res    = {}\n",
    "    dtrain = xgb.DMatrix(X)\n",
    "    dtrain.set_float_info(\"label_lower_bound\",y_lower.values)\n",
    "    dtrain.set_float_info(\"label_upper_bound\",y_upper.values)\n",
    "\n",
    "    dtest  = xgb.DMatrix(X_val)\n",
    "    dtest.set_float_info(\"label_lower_bound\",y_lower_val.values)\n",
    "    dtest.set_float_info(\"label_upper_bound\",y_upper_val.values)\n",
    "    \n",
    "    bst    = xgb.train(params,dtrain,num_boost_round=num_round,evals=[(dtrain,\"train\"),(dtest,\"test\")],evals_result=res,verbose_eval=False)\n",
    "    min_val_error = round(np.min(res['test'][distributionCol]),4)\n",
    "    return(min_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(distribution,trial):\n",
    "    SEED         = 1\n",
    "    Kfolds       = KFold(n_splits=5,shuffle=True,random_state=SEED)\n",
    "    num_round    = 5000\n",
    "    res          = 0\n",
    "    # Discrete-uniform parameter\n",
    "    eta              = trial.suggest_discrete_uniform('eta',0.001,1.001,0.1)\n",
    "    max_depth        = trial.suggest_discrete_uniform('max_depth',2, 10,2)\n",
    "    min_child_weight = trial.suggest_discrete_uniform('min_child_weight',0.1,100.1,10)\n",
    "    reg_alpha        = trial.suggest_loguniform('reg_alpha',0.0001,100)\n",
    "    reg_lambda       = trial.suggest_loguniform('reg_lambda',0.0001,100)\n",
    "#     sigma            = trial.suggest_discrete_uniform('sigma',1,100,1)\n",
    "#     distribution     = trial.suggest_categorical('distribution',['normal','logistic','extreme'])\n",
    "    sigma            = 1\n",
    "    \n",
    "    distribution_sigma = distribution+ ',' + str(sigma)\n",
    "    eval_metric     = 'aft-nloglik@'+distribution_sigma\n",
    "    base_score      = 0.5\n",
    "    \n",
    "    params   = {\n",
    "                'eta':eta,\n",
    "                'max_depth':int(max_depth),\n",
    "                'min_child_weight':min_child_weight,\n",
    "                'subsample':0.7,\n",
    "                'reg_alpha':reg_alpha,\n",
    "                'reg_lambda':reg_lambda,\n",
    "                'aft_noise_distribution' : distribution, \n",
    "                'aft_sigma': sigma,\n",
    "                'eval_metric':eval_metric,\n",
    "                'base_score':base_score,\n",
    "                'objective':\"aft:survival\",\n",
    "                'verbosity': 0\n",
    "                }\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(Kfolds.split(X, y_lower,y_upper)):\n",
    "        tr_x, tr_y_lower,tr_y_upper = X.iloc[trn_idx,:],y_lower.iloc[trn_idx],y_upper.iloc[trn_idx]\n",
    "        vl_x, vl_y_lower,vl_y_upper = X.iloc[val_idx,:], y_lower.iloc[val_idx],y_upper.iloc[val_idx]\n",
    "        res = res + trainModel(tr_x,vl_x,tr_y_lower,tr_y_upper,vl_y_lower,vl_y_upper,params,num_round,distribution_sigma)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_iter(eta,max_depth,min_child_weight,reg_alpha,reg_lambda,sigma,distribution): \n",
    "    SEED          = 1\n",
    "    Kfolds        = KFold(n_splits=5,shuffle=True,random_state=SEED)\n",
    "    num_round     = 5000\n",
    "    # Discrete-uniform parameter\n",
    "    distributionCol = distribution+ ',' + str(sigma)\n",
    "    eval_metric     = 'aft-nloglik@'+distributionCol\n",
    "    base_score      = 0.5\n",
    "    \n",
    "    params   = {\n",
    "                'eta':eta,\n",
    "                'max_depth':int(max_depth),\n",
    "                'min_child_weight':min_child_weight,\n",
    "                'subsample':0.7,\n",
    "                'reg_alpha':reg_alpha,\n",
    "                'reg_lambda':reg_lambda,\n",
    "                'aft_noise_distribution' : distribution, \n",
    "                'aft_sigma': sigma,\n",
    "                'eval_metric':eval_metric,\n",
    "                'base_score':base_score,\n",
    "                'objective':\"aft:survival\",\n",
    "                'verbosity': 0\n",
    "                }\n",
    "\n",
    "    res_data = pd.DataFrame()\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(Kfolds.split(X, y_lower,y_upper)):\n",
    "        tr_x, tr_y_lower,tr_y_upper = X.iloc[trn_idx,:],y_lower.iloc[trn_idx],y_upper.iloc[trn_idx]\n",
    "        vl_x, vl_y_lower,vl_y_upper = X.iloc[val_idx,:], y_lower.iloc[val_idx],y_upper.iloc[val_idx]\n",
    "        res_data[fold_] = trainModelIter(tr_x,vl_x,tr_y_lower,tr_y_upper,vl_y_lower,vl_y_upper,params,num_round,distributionCol)\n",
    "    res_data['total'] = res_data.sum(axis=1)\n",
    "    res = {}\n",
    "    num_round = res_data.idxmin(axis=0, skipna=True)['total']\n",
    "    res['num_round'] = num_round\n",
    "    res['min_val_error'] = min(res_data['total'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_domain = ['ATAC_JV_adipose','CTCF_TDH_ENCODE','H3K27ac-H3K4me3_TDHAM_BP','H3K27ac_TDH_some','H3K36me3_AM_immune']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data      = data_import(data_name_domain[4])\n",
    "data_name = data_name_domain[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['inputs']\n",
    "labels = data['labels']\n",
    "folds  = data['folds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = data_massage(inputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global X\n",
    "global X_val\n",
    "global y_lower\n",
    "global y_upper\n",
    "global y_upper_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 extreme\n"
     ]
    }
   ],
   "source": [
    "for fold in np.unique(folds['fold'].values):\n",
    "    start        = time.time()\n",
    "    res          = getXY(fold,folds,inputs,labels)\n",
    "    X            = res['X']        \n",
    "    X_val        = res['X_val']\n",
    "    y_lower      = res['y_lower']\n",
    "    y_lower_val  = res['y_lower_val']\n",
    "    y_upper      = res['y_upper']\n",
    "    y_upper_val  = res['y_upper_val']\n",
    "    \n",
    "    for distribution in ['normal','logistic','extreme']:\n",
    "        \n",
    "        print(fold,distribution)\n",
    "        sampler = TPESampler(seed=1)  # Make the sampler behave in a deterministic way.\n",
    "        study = optuna.create_study(sampler=sampler)\n",
    "        study.optimize(functools.partial(objective,distribution), n_trials=100)\n",
    "        trial         = study.best_trial\n",
    "        json_filename = \"../../../../result/\"+data_name+\"/xgboost/fold\"+str(fold)+'_'+distribution+'_param.json'\n",
    "        with open(json_filename, \"w\") as write_file:\n",
    "            json.dump(trial.params, write_file)\n",
    "            \n",
    "    end            = time.time()\n",
    "    time_taken     = end - start\n",
    "    run_time[fold] = time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time1 = {}\n",
    "for key in run_time.keys():\n",
    "    run_time1[str(key)] = run_time[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"../../../../result/\"+data_name+\"/xgboost/run_time_tuning1.json\"\n",
    "with open(json_filename, \"w\") as write_file:\n",
    "    json.dump(run_time1, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelIter(X,X_val,y_lower,y_upper,y_lower_val,y_upper_val,params,num_round,distributionCol):\n",
    "    \n",
    "    res    = {}\n",
    "    dtrain = xgb.DMatrix(X)\n",
    "    dtrain.set_float_info(\"label_lower_bound\",y_lower)\n",
    "    dtrain.set_float_info(\"label_upper_bound\",y_upper)\n",
    "\n",
    "    dtest  = xgb.DMatrix(X_val)\n",
    "    dtest.set_float_info(\"label_lower_bound\",y_lower_val)\n",
    "    dtest.set_float_info(\"label_upper_bound\",y_upper_val)\n",
    "\n",
    "    bst    = xgb.train(params,dtrain,num_boost_round=num_round,evals=[(dtrain,\"train\"),(dtest,\"test\")],evals_result=res,verbose_eval=False)\n",
    "    val_error = res['test'][distributionCol]\n",
    "    \n",
    "    return(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold in range(2,3):\n",
    "for fold in np.unique(folds['fold'].values):\n",
    "    start_time   = time.time()\n",
    "    res = getXY(fold,folds,inputs,labels)\n",
    "    X            = res['X']        \n",
    "    X_val        = res['X_val']\n",
    "    y_lower      = res['y_lower']\n",
    "    y_lower_val  = res['y_lower_val']\n",
    "    y_upper      = res['y_upper']\n",
    "    y_upper_val  = res['y_upper_val']\n",
    "    \n",
    "    for distribution in ['normal','logistic','extreme']:\n",
    "        json_filename = \"../../../../result/\"+data_name+\"/xgboost/fold\"+str(fold)+'_'+distribution+'_param.json'\n",
    "        with open(json_filename, errors='ignore') as json_data:\n",
    "            json_fold = json.load(json_data, strict=False)\n",
    "        eta = json_fold['eta']\n",
    "        max_depth = json_fold['max_depth']\n",
    "        min_child_weight = json_fold['min_child_weight']\n",
    "        reg_alpha = json_fold['reg_alpha']\n",
    "        reg_lambda = json_fold['reg_lambda']\n",
    "        sigma= 1\n",
    "        res      = best_iter(eta,max_depth,min_child_weight,reg_alpha,reg_lambda,sigma,distribution)\n",
    "        new_json = {}\n",
    "        new_json['eta'] = eta\n",
    "        new_json['max_depth'] = max_depth\n",
    "        new_json['min_child_weight'] = min_child_weight\n",
    "        new_json['reg_alpha']     = reg_alpha\n",
    "        new_json['reg_lambda']    = reg_lambda\n",
    "        new_json['sigma']         = sigma\n",
    "        new_json['distribution']  = distribution\n",
    "        new_json['num_round']     = int(res['num_round'])\n",
    "        if res['min_val_error'] == float('inf'):\n",
    "            res['min_val_error'] = 10**8\n",
    "        new_json['min_val_error'] = res['min_val_error']\n",
    "        json_filename = \"../../../../result/\"+data_name+\"/xgboost/fold_new\"+str(fold)+'_'+distribution+'_param.json'\n",
    "        with open(json_filename, \"w\") as write_file:\n",
    "             json.dump(new_json, write_file)\n",
    "    end_time        = time.time()\n",
    "    time_taken      = end_time - start_time\n",
    "    run_time2[fold] = time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold in range(2,3):\n",
    "for fold in np.unique(folds['fold'].values):\n",
    "    fold_data = pd.DataFrame()\n",
    "    for distribution in ['normal','logistic','extreme']:\n",
    "        json_filename = \"../../../../result/\"+data_name+\"/xgboost/fold_new\"+str(fold)+'_'+distribution+'_param.json'\n",
    "        with open(json_filename, errors='ignore') as json_data:\n",
    "            json_fold = json.load(json_data, strict=False)\n",
    "        dist_data = pd.DataFrame.from_dict(json_fold,orient='index',columns=[distribution])\n",
    "        fold_data = pd.concat([fold_data,dist_data],axis=1)\n",
    "    fold_data = fold_data.transpose()\n",
    "    fold_data['min_val_error'] = fold_data['min_val_error'].astype('float')\n",
    "    best_dis   = fold_data['min_val_error'].idxmin()\n",
    "    best_param = fold_data.loc[best_dis]\n",
    "    json_filename = \"../../../../result/\"+data_name+\"/xgboost/fold_new\"+str(fold)+'_dis'+'_param.json'\n",
    "    best_param = best_param.to_dict()\n",
    "    with open(json_filename, \"w\") as write_file:\n",
    "        json.dump(best_param, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time3 ={}\n",
    "for key in run_time2.keys():\n",
    "    run_time3[str(key)] = run_time2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"../../../../result/\"+data_name+\"/xgboost/run_time_tuning2.json\"\n",
    "with open(json_filename, \"w\") as write_file:\n",
    "    json.dump(run_time3, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
